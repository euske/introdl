<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" type="text/css" href="../common.css" />
<title>第4回 ディープラーニングへの入り口: MNIST
/ 真面目なプログラマのためのディープラーニング入門</title>
<style><!--
.example { outline: 1px solid black; }
.bc { color: blue; font-weight: bold; }
.ew { color: red; font-weight: bold; }
.prev { color: gray; }
--></style>
<body>
<div class=nav>
<a href="../index.html">&lt; もどる</a>
</div>

<h1>第4回 ディープラーニングへの入り口: MNIST</h1>

<ol>
<li> <a href="#dl-what">ディープラーニングとは</a>
<li> <a href="#mnist-what">MNIST (手書き文字認識) とは</a>
<li> <a href="#numpy">NumPy入門</a>
  <ul>
  <li> <a href="#numpy-install">NumPy を使う準備</a>
  <li> <a href="#numpy-ndarray">ndarray型とは</a>
  <li class=ex> <a href="#ex4-1">演習4-1. ndarray配列を作成する</a>
  <li> <a href="#numpy-op">ndarray型における演算</a>
  <li class=ex> <a href="#ex4-2">演習4-2. ndarray型の演算</a>
  <li> <a href="#numpy-assign">ndarray型の参照・変更</a>
  <li> <a href="#numpy-utils">ndarray型を便利に使うための関数</a>
  </ul>
<li> <a href="#nn-numpy">NumPyを使ったニューラルネットワークの実装</a>
  <ul>
  <li class=ex> <a href="#ex4-3">演習4-3. NumPyを使ったニューラルネットワークを実行する</a>
  </ul>
<li> <a href="#mnist-impl">MNIST を実装する</a>
  <ul>
  <li> <a href="#mnist-minibatch">ミニバッチと SGD法</a>
  <li class=ex> <a href="#ex4-4">演習4-4. MNISTを学習する</a>
  <li> <a href="#mnist-argmax">学習結果を使う</a>
  <li class=ex> <a href="#ex4-5">演習4-5. MNISTの正解数を測定する</a>
  <li> <a href="#mnist-validation">エポックは何回やれば充分か?</a>
  <li> <a href="#mnist-softmax">Softmax 活性化関数と交差エントロピー損失</a>
  <li class=ex> <a href="#ex4-6">演習4-6. Softmax関数と交差エントロピー誤差を求める</a>
  <li class=ex> <a href="#ex4-7">演習4-7. Softmaxレイヤーを使った MNIST</a>
  <li> <a href="#mnist-logsoftmax">LogSoftmax 関数</a>
  </ul>
<li> <a href="#summary">まとめ</a>
</ol>


<h2 id="dl-what">1. ディープラーニングとは</h2>
<p>
<u>ディープ ニューラル ネットワーク</u> (deep neural network) とは、
レイヤーを非常に多く (深く) 重ねたニューラルネットワークのことである。
<u>ディープ ラーニング</u> (深層学習、deep learning) も
ほぼ同じ意味で使われている。
<p>
ディープラーニングは、もともと画像処理の分野で発展してきたものである。
現在では音声やテキスト処理にも応用されているが、画像処理での利用が多い。
ディープラーニングと従来の機械学習方式との違いは、
従来の機械学習の多くがデータに関するなんらかの前処理 (輪郭抽出など) が
必要だったのに対して、ディープラーニングはその前処理工程もふくめて
学習できるため「生のデータをそのまま入力できる」点であるとされる。

<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="410" height="210">
<defs>
  <marker id="arrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="black" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="black" stroke-width="2">
  <rect x="110" y="10" width="80" height="80" />
  <rect x="220" y="10" width="80" height="80" />
  <path d="M130,120 l-20,0 l0,80 l20,0" />
  <path d="M155,120 l-20,0 l0,80 l20,0" />
  <path d="M180,120 l-20,0 l0,80 l20,0" />
  <path d="M205,120 l-20,0 l0,80 l20,0" />
  <path d="M230,120 l-20,0 l0,80 l20,0" />
  <path d="M255,120 l-20,0 l0,80 l20,0" />
  <rect x="260" y="120" width="40" height="80" />
  <ellipse cx="50" cy="110" rx="40" ry="80" />
  <ellipse cx="360" cy="110" rx="40" ry="80" />
</g>
<rect x="150" y="145" width="100" height="30" fill="white" stroke="none" />
<g fill="none" stroke="black" stroke-width="4" marker-end="url(#arrow)">
  <path d="M80,50 l20,0" />
  <path d="M190,50 l20,0" />
  <path d="M300,50 l20,0" />
  <path d="M80,160 l20,0" />
  <path d="M300,160 l20,0" />
</g>
<g style="font-size:75%;" text-anchor="middle">
  <text x="50" y="110" dy="0.5em">生のデータ</text>
  <text x="360" y="110" dy="0.5em">出力</text>
  <text x="150" y="50" dy="0.5em">前処理</text>
  <text x="260" y="50" dy="-0.5em">従来の</text>
  <text x="260" y="50" dy="0.7em">機械学習</text>
  <text x="200" y="165" dy="-0.5em">ディープ</text>
  <text x="200" y="165" dy="0.5em">ラーニング</text>
</g>
</svg><br>
従来の機械学習とディープラーニングの違い
</div>

<p>
今回はディープラーニングの基礎を学ぶために、
代表的な 2つの認識タスクである MNIST と CIFAR-10 を
実装してみよう。その過程で、認識精度を上げ、
より実用的なニューラルネットワークを設計するための
いくつかのテクニックについて説明する。


<h2 id="mnist-what">2. MNIST (手書き文字認識) とは</h2>
<p>
<a target="_blank" href="http://yann.lecun.com/exdb/mnist/">MNISTデータベース</a> は
手書き文字認識のためのデータセットで、
ディープラーニングの入門書で必ずといってよいほど扱われている。
MNIST を使った手書き文字認識は、様々な機械学習アルゴリズムの
性能を試験するときの指標のひとつとなっている。
<p>
ここでは入力と出力は以下のように定義される:
<ul>
<li> 入力: 28×28 (=784) ピクセルからなるグレースケール画像
<li> 出力: <code>0</code> から <code>9</code> までの「ラベル」
</ul>
ニューラルネットワークでは、10種類のラベルを区別するために
10個の値の出力を使うことにする。これは
「<strong>特定の値だけが <code>1</code> で、それ以外は <code>0</code> である</strong>」
ようなベクトルである。
このようなベクトルは、<u>one-hot ベクトル</u> などと呼ばれる:
<div class=example>
<ul>
<li> 入力: <img src="digit3.png"> (784要素のベクトル)
<li> 出力: <code>[0 0 0 1 0 0 0 0 0 0]</code> (10要素のベクトル、3番目の要素が 1)
</ul>
<ul>
<li> 入力: <img src="digit9.png"> (784要素のベクトル)
<li> 出力: <code>[0 0 0 0 0 0 0 0 0 1]</code> (10要素のベクトル、9番目の要素が 1)
</ul>
</div>

<p>
このタスクは前回実装したニューラルネットワークを使って、
簡単に実行することができる:
<ol>
<li> 入力が 784個で、出力が 10個の値をもつニューラルネットワークを作る。
<li> 訓練データの画像とラベルを使って学習する:
  <ul>
    <li> 28×28ピクセルの画像を 784要素のフラットなリストに変換し、
      グレースケールの値 (0〜255) を 0〜1 に変換して入力する。
    <li> 各ラベルは 10要素のリストに変換し、
      10個の出力と比較して損失を計算する。
  </ul>
</ol>

<p>
さて、2次元の画像を 1次元のリストに変換してしまっているが、
これでは上下左右の関係が学習できないのでは? と思うかもしれない。
しかし、たとえば以下のような対応関係があったとして:

<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="160" height="130">
<g fill="none" stroke="black" stroke-width="2">
  <rect x="40" y="15" width="110" height="110" />
  <path d="M40,35 l110,0 M40,55 l110,0 M40,105 l110,0" />
  <path d="M60,15 l0,110 M80,15 l0,110 M130,15 l0,110" />
</g>
<g style="font-size:75%;" text-anchor="middle">
  <text x="50" y="10">0</text>
  <text x="70" y="10">1</text>
  <text x="90" y="10">2</text>
  <text x="110" y="10">...</text>
  <text x="140" y="10">27</text>
  <text x="37" y="30" text-anchor="end">(0,0)</text>
  <text x="37" y="50" text-anchor="end">(0,1)</text>
  <text x="37" y="70" text-anchor="end">(0,2)</text>
  <text x="37" y="90" text-anchor="end">...</text>
  <text x="37" y="120" text-anchor="end">(0,27)</text>
</g>
<g style="font-size:50%; font-weight: bold;" text-anchor="middle">
  <text x="50" y="30">0</text>
  <text x="70" y="30">1</text>
  <text x="90" y="30">...</text>
  <text x="140" y="30">27</text>
  <text x="50" y="50">28</text>
  <text x="70" y="50">29</text>
  <text x="90" y="50">...</text>
  <text x="140" y="50">55</text>
  <text x="50" y="120">756</text>
  <text x="70" y="120">757</text>
  <text x="90" y="120">...</text>
  <text x="140" y="120">783</text>
</g>
</svg><br>
画像と各要素との対応
</div>
<p>
ここで左右のピクセル関係は、たとえば
「0番目の要素と 1番目の要素の関係」であり、
上下のピクセル関係は「0番目の要素と 28番目の要素の関係」などに相当する。
ニューラルネットワークのノードは全接続されているので、
位置に関係なく「すべてのピクセル間の関係」を学習しようとする。
したがって、画像サイズがつねに一定で、各ピクセルがつねに
同じ要素に対応していれば、この方法でも十分に上下左右のピクセル間の関係を
学習できることになる。
このように、どんなデータでもベクトルに変換して学習できることは
ニューラルネットワークの大きな特長である。


<h2 id="numpy">3. NumPy入門</h2>
<p>
前回のニューラルネットワークの実装では、
数値の表現に Python のリストを使っていた。
これをそのまま使うこともできるが、効率が悪いため
MNIST の認識をおこなう前に <a target="_blank" href="https://numpy.org/">NumPy</a> と呼ばれる
Python ライブラリを使って、コードをもう少し高速化する。
NumPy を使うと Python 上で固定長の数値配列を簡単かつ高速に演算することができる。

<h3 id="numpy-install">3.1. NumPy を使う準備</h3>
<p>
まず numpy モジュールは素の Python には含まれていないので、
インストールする必要がある (Anaconda あるいは Google Colab を使っている場合は不要):
<blockquote><pre>
C:\&gt; <strong>pip install numpy</strong>
</pre></blockquote>
<p>
Python 中でモジュールを使うときは、
以下のように <code>import</code> する (なぜか numpy を np という名前にするのが慣例):
<blockquote><pre>
import numpy as np
</pre></blockquote>

<h3 id="numpy-ndarray">3.2. ndarray型とは</h3>
<p>
NumPy が提供する機能は、基本的には <code>ndarray</code>型だけである。
これは通常の Python リストと似ているが、以下の点が異なっている:
<ul>
<li> すべての要素は同一の数値型
  (<code>int</code>型や <code>float</code>型) である。
<li> 要素の追加・削除はできず、固定長として扱う。
<li> Python リストでは <code>+</code> 演算子はリストの連結を
  意味するが、<code>ndarray</code>型はそうではない。(後述)
</ul>
<p>
<code>ndarray</code>配列を作成するには、以下の方法のどれかを使う:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([1,2,3,4])</strong>           <span class=comment># 4要素のPythonリストからndarrayを作成</span>
&gt;&gt;&gt; <strong>np.array([[1,2,3], [4,5,6]])</strong>  <span class=comment># 2×3要素のPythonリストからndarrayを作成</span>
&gt;&gt;&gt; <strong>np.zeros(4)</strong>                   <span class=comment># 4要素すべてゼロ</span>
&gt;&gt;&gt; <strong>np.zeros((2, 3))</strong>              <span class=comment># 2列3行すべてゼロ</span>
&gt;&gt;&gt; <strong>np.random.random(4)</strong>           <span class=comment># 4要素の乱数 (0〜1の範囲)</span>
&gt;&gt;&gt; <strong>np.random.random((2, 3))</strong>      <span class=comment># 2列3行の乱数 (0〜1の範囲)</span>
</pre></blockquote>

<div class=exercise id="ex4-1">
<div class=header>演習4-1. ndarray配列を作成する</div>
<ol type=a>
<li> Pythonで10×10要素のリスト(のリスト) を作り、
  これを <code>ndarray</code>配列に変換せよ。
<li> <code>np.random.random((3, 3))</code> の値を表示せよ。
</ol>
</div>

<h3 id="numpy-op">3.3. ndarray型における演算</h3>
<p>
<code>ndarray</code>型は Python のリストに比べ、
演算がより簡単に行えるよう拡張されている。
ここでは代表的な例をいくつか説明する。
<p>
まず単一の値と <code>ndarray</code>配列を演算すると、
その値は配列中の各要素に分配 (<span class=bc>broadcast</span>) される:
<blockquote><pre>
&gt;&gt;&gt; <strong>5 + np.array([1,2,3])</strong>
array([6, 7, 8])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="300" height="40">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="5" y="15" width="20" height="20" />
  <rect x="175" y="15" width="20" height="20" />
  <rect x="210" y="15" width="20" height="20" />
  <rect x="245" y="15" width="20" height="20" />
  <path d="M25,15 c50,-10,100,-10,145,-2" marker-end="url(#barrow)" />
  <path d="M25,15 c60,-12,120,-12,180,-2" marker-end="url(#barrow)" />
  <path d="M25,15 c70,-15,140,-15,215,-2" marker-end="url(#barrow)" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">5 + np.array([1, 2, 3])</text>
</g>
</svg><br>
ひとつの値が <code>ndarray</code> 中のすべての要素に分配される。
</div></blockquote>

<p>
これは演算子の左右が逆転しても同じである:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([1,2,3]) * 5</strong>
array([ 5, 10, 15])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="300" height="40">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="270" y="15" width="20" height="20" />
  <rect x="125" y="15" width="20" height="20" />
  <rect x="160" y="15" width="20" height="20" />
  <rect x="195" y="15" width="20" height="20" />
  <path d="M270,15 c-40,-15,-80,-15,-120,-2" marker-end="url(#barrow)" />
  <path d="M270,15 c-30,-12,-60,-12,-85,-2" marker-end="url(#barrow)" />
  <path d="M270,15 c-20,-10,-40,-10,-50,-2" marker-end="url(#barrow)" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">np.array([1, 2, 3]) * 5</text>
</g>
</svg><br>
右側の値が <code>ndarray</code> のすべての要素に分配される。
</div></blockquote>

<p>
さて、通常の Python リストとは異なり、<code>ndarray</code>配列どうしは
<strong><code>+</code> 演算子では連結できない</strong>。
<code>+</code> 演算子は<strong>要素ごとの計算</strong> (<span class=ew>element-wise</span>)
として解釈される。
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([1,2,3]) + np.array([4,5,6])</strong>
array([5, 7, 9])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="500" height="40">
<g fill="none" stroke="red" stroke-width="2">
  <rect x="125" y="15" width="20" height="20" />
  <rect x="160" y="15" width="20" height="20" />
  <rect x="195" y="15" width="20" height="20" />
  <rect x="390" y="15" width="20" height="20" />
  <rect x="425" y="15" width="20" height="20" />
  <rect x="460" y="15" width="20" height="20" />
  <path d="M145,15 c50,-15,195,-15,245,0" />
  <path d="M180,15 c50,-15,195,-15,245,0" />
  <path d="M215,15 c50,-15,195,-15,245,0" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">np.array([1, 2, 3]) + np.array([4, 5, 6])</text>
</g>
</svg><br>
各要素ごとに値が計算される。
</div></blockquote>

<p>
他の <code>*</code>, <code>/</code> などの演算子でも同様である:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([1,2,3]) * np.array([4,5,6])</strong>
array([4, 10, 18])
</pre></blockquote>

<p>
ただし、このとき 2つの <code>ndarray</code>配列は
<strong>同じ要素数でなければならない</strong>:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([1,2,3]) + np.array([4,5,6,7,8])</strong>
<span class=err>ValueError: 長さが異なるものは演算不可</span>
</pre></blockquote>

<p>
さらに注意が必要なのは、多次元配列 (2次元以上) の場合である。
NumPy では
多次元配列は「配列の配列 (<code>ndarray</code> の <code>ndarray</code>)」として
表現されているが、多次元配列の要素にも同様のルールが再帰的に適用されるのである:
<blockquote><pre>
&gt;&gt;&gt; <strong>5 + np.array([[1,2,3], [4,5,6]])</strong>
array([[ 6,  7,  8],
       [ 9, 10, 11]])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="520" height="70">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="5" y="15" width="20" height="20" />
  <rect x="180" y="15" width="105" height="20" />
  <rect x="180" y="45" width="105" height="20" />
  <rect x="355" y="15" width="20" height="20" />
  <rect x="410" y="15" width="20" height="20" />
  <rect x="445" y="15" width="20" height="20" />
  <rect x="480" y="15" width="20" height="20" />
  <rect x="355" y="45" width="20" height="20" />
  <rect x="410" y="45" width="20" height="20" />
  <rect x="445" y="45" width="20" height="20" />
  <rect x="480" y="45" width="20" height="20" />
  <path d="M25,20 c50,-15,100,-15,150,-8" marker-end="url(#barrow)" />
  <path d="M25,20 c50,-5,100,0,150,25" marker-end="url(#barrow)" />
  <path d="M375,15 c10,-5,20,-5,30,-2" marker-end="url(#barrow)" />
  <path d="M375,15 c20,-8,40,-8,65,-2" marker-end="url(#barrow)" />
  <path d="M375,15 c30,-10,60,-10,100,-2" marker-end="url(#barrow)" />
  <path d="M375,45 c10,-5,20,-5,30,-2" marker-end="url(#barrow)" />
  <path d="M375,45 c20,-8,40,-8,65,-2" marker-end="url(#barrow)" />
  <path d="M375,45 c30,-10,60,-10,100,-2" marker-end="url(#barrow)" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">5 + np.array([[1, 2, 3],</text>
  <text x="180" y="60">[4, 5, 6]])</text>
  <text x="360" y="30">5</text>
  <text x="400" y="30">[1, 2, 3]</text>
  <text x="360" y="60">5</text>
  <text x="400" y="60">[4, 5, 6]</text>
</g>
</svg><br>
まず 5 が np.array の 2要素に分配され (<span class=bc>broadcast</span>)、
さらに内側の 3要素にも分配される (<span class=bc>broadcast</span>)。
</div></blockquote>

<p>
<span class=ew>Element-wise</span> と
<span class=bc>broadcast</span> が両方適用される場合もある。
以下の例はやや変則的で、1要素の配列がひとつの値と同様に扱われている:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([[-1],[1]]) * np.array([[1,2,3], [4,5,6]])</strong>
array([[-1, -2, -3],
       [ 4,  5,  6]])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="750" height="70">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="red" stroke-width="2">
  <rect x="135" y="15" width="40" height="20" />
  <rect x="345" y="15" width="105" height="20" />
  <rect x="135" y="45" width="40" height="20" />
  <rect x="345" y="45" width="105" height="20" />
  <path d="M175,15 c50,-10,120,-10,170,0" />
  <path d="M175,45 c50,-10,120,-10,170,0" />
</g>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="545" y="15" width="40" height="20" />
  <rect x="630" y="15" width="20" height="20" />
  <rect x="665" y="15" width="20" height="20" />
  <rect x="700" y="15" width="20" height="20" />
  <rect x="545" y="45" width="40" height="20" />
  <rect x="630" y="45" width="20" height="20" />
  <rect x="665" y="45" width="20" height="20" />
  <rect x="700" y="45" width="20" height="20" />
  <path d="M585,15 c10,-5,20,-5,40,-2" marker-end="url(#barrow)" />
  <path d="M585,15 c20,-8,40,-8,75,-2" marker-end="url(#barrow)" />
  <path d="M585,15 c30,-10,60,-10,110,-2" marker-end="url(#barrow)" />
  <path d="M585,45 c10,-5,20,-5,40,-2" marker-end="url(#barrow)" />
  <path d="M585,45 c20,-8,40,-8,75,-2" marker-end="url(#barrow)" />
  <path d="M585,45 c30,-10,60,-10,110,-2" marker-end="url(#barrow)" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">np.array([[-1],</text>
  <text x="135" y="60">[1])</text>
  <text x="200" y="30">* np.array([[1, 2, 3],</text>
  <text x="345" y="60">[4, 5, 6]])</text>
  <text x="540" y="30">[-1]</text>
  <text x="620" y="30">[1, 2, 3]</text>
  <text x="540" y="60">[1]</text>
  <text x="620" y="60">[4, 5, 6]</text>
</g>
</svg><br>
2つの np.array が各要素ごとに対応し (<span class=ew>element-wise</span>)、
つぎに各配列どうしの演算が行われる (<span class=bc>broadcast</span>)。
</div></blockquote>
<p>
配列の次数が異なる場合 (1次元 + 2次元) でも
<span class=bc>broadcast</span> が適用される:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([-1,0,1]) + np.array([[1,2,3], [4,5,6]])</strong>
array([[0, 2, 4],
       [3, 5, 7]])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="800" height="70">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="120" y="15" width="95" height="20" />
  <rect x="360" y="15" width="105" height="20" />
  <rect x="360" y="45" width="105" height="20" />
  <path d="M215,20 c50,-15,90,-15,140,-8" marker-end="url(#barrow)" />
  <path d="M215,20 c50,-5,90,0,140,25" marker-end="url(#barrow)" />
</g>
<g fill="none" stroke="red" stroke-width="2">
  <rect x="555" y="15" width="20" height="20" />
  <rect x="585" y="15" width="20" height="20" />
  <rect x="610" y="15" width="20" height="20" />
  <rect x="690" y="15" width="20" height="20" />
  <rect x="725" y="15" width="20" height="20" />
  <rect x="760" y="15" width="20" height="20" />
  <rect x="555" y="45" width="20" height="20" />
  <rect x="585" y="45" width="20" height="20" />
  <rect x="610" y="45" width="20" height="20" />
  <rect x="690" y="45" width="20" height="20" />
  <rect x="725" y="45" width="20" height="20" />
  <rect x="760" y="45" width="20" height="20" />
  <path d="M575,15 c30,-10,80,-10,115,0" />
  <path d="M605,15 c30,-10,90,-10,120,0" />
  <path d="M630,15 c30,-10,100,-10,130,0" />
  <path d="M575,45 c30,-10,80,-10,115,0" />
  <path d="M605,45 c30,-10,90,-10,120,0" />
  <path d="M630,45 c30,-10,100,-10,130,0" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">np.array([-1,0,1])+np.array([[1, 2, 3],</text>
  <text x="360" y="60">[4, 5, 6]])</text>
  <text x="540" y="30">[-1,0,1]</text>
  <text x="680" y="30">[1, 2, 3]</text>
  <text x="540" y="60">[-1,0,1]</text>
  <text x="680" y="60">[4, 5, 6]</text>
</g>
</svg><br>
まず左側の np.array が 2要素に分配され (<span class=bc>broadcast</span>)、
つぎに各配列どうしの演算が行われる (<span class=ew>element-wise</span>)。
</div></blockquote>
<p>
(なお、NumPy の用語では「次元 (=ひとつの要素を特定するのに必要な添字の数)」
のことを <u>axis</u> と呼んでいる。)

<p>
以下の例は少しわかりにくいかもしれない。
<span class=bc>broadcast</span> の結果、全体の要素数が増えることもある:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.array([[-1],[1]]) * np.array([1,2,3])</strong>
array([[-1, -2, -3],
       [ 1,  2,  3]])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="750" height="70">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="135" y="15" width="40" height="20" />
  <rect x="335" y="15" width="105" height="20" />
  <rect x="135" y="45" width="40" height="20" />
  <path d="M335,20 c-50,-15,-100,-15,-155,-8" marker-end="url(#barrow)" />
  <path d="M335,20 c-50,-5,-100,0,-155,25" marker-end="url(#barrow)" />
</g>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="545" y="15" width="40" height="20" />
  <rect x="630" y="15" width="20" height="20" />
  <rect x="665" y="15" width="20" height="20" />
  <rect x="700" y="15" width="20" height="20" />
  <rect x="545" y="45" width="40" height="20" />
  <rect x="630" y="45" width="20" height="20" />
  <rect x="665" y="45" width="20" height="20" />
  <rect x="700" y="45" width="20" height="20" />
  <path d="M585,15 c10,-5,20,-5,40,-2" marker-end="url(#barrow)" />
  <path d="M585,15 c20,-8,40,-8,75,-2" marker-end="url(#barrow)" />
  <path d="M585,15 c30,-10,60,-10,110,-2" marker-end="url(#barrow)" />
  <path d="M585,45 c10,-5,20,-5,40,-2" marker-end="url(#barrow)" />
  <path d="M585,45 c20,-8,40,-8,75,-2" marker-end="url(#barrow)" />
  <path d="M585,45 c30,-10,60,-10,110,-2" marker-end="url(#barrow)" />
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="10" y="30">np.array([[-1],</text>
  <text x="135" y="60">[1])</text>
  <text x="200" y="30">* np.array([1, 2, 3])</text>
  <text x="540" y="30">[-1]</text>
  <text x="620" y="30">[1, 2, 3]</text>
  <text x="540" y="60">[1]</text>
  <text x="620" y="60">[1, 2, 3]</text>
</g>
</svg><br>
右側の np.array が左側の 2要素に分配され (<span class=bc>broadcast</span>)、
さらに各要素が配列に分配される (<span class=bc>broadcast</span>)。
</div></blockquote>

<div class=exercise id="ex4-2">
<div class=header>演習4-2. ndarray型の演算</div>
<p>
以下の <code>ndarray</code>配列の演算をしたときの結果を予想し、
実際に実行してみて結果を確認せよ。
<pre>
&gt;&gt;&gt; <strong>np.array([1,2]) * np.array([3,4])</strong>
&gt;&gt;&gt; <strong>np.array([1,2]) * 4</strong>
&gt;&gt;&gt; <strong>np.array([[1,2], [3,4]]) + np.array([[5,6], [7,8]])</strong>
&gt;&gt;&gt; <strong>np.array([[1],[2],[3]]) * np.array([1,2,3])</strong>
&gt;&gt;&gt; <strong>np.array([1,2]) * np.array([3])</strong>
&gt;&gt;&gt; <strong>np.array([1,2]) * np.array([3, 4, 5])</strong>
</pre>
</div>

<div class=figure>
<img width="260" height="138" src="broadcast.png">
</div>

<h3 id="numpy-assign">3.4. ndarray型の参照・変更</h3>
<p>
<code>ndarray</code>配列の参照・変更は、基本的に Python リストと
同じように扱える。多次元配列の <code>ndarray</code> の場合、
<code>a[i][j]</code> とともに <code>a[i,j]</code> という表記も許されている。
<blockquote><pre>
&gt;&gt;&gt; <strong>a = np.array([[1,2,3], [4,5,6]])</strong>
&gt;&gt;&gt; <strong>a[0]</strong>        <span class=comment># 0行目を取得。</span>
array([1, 2, 3])
&gt;&gt;&gt; <strong>a[1][2]</strong>     <span class=comment># 1行2列目の値を取得。</span>
6
&gt;&gt;&gt; <strong>a[1][1:3]</strong>   <span class=comment># 1行1〜2列目の値を取得。</span>
array([5, 6])
&gt;&gt;&gt; <strong>a[1,2]</strong>      <span class=comment># 上と同じ。</span>
6
&gt;&gt;&gt; <strong>a[0,1] = 0</strong>  <span class=comment># 0行1列目の値を変更。</span>
&gt;&gt;&gt; <strong>a</strong>
array([[1, 0, 3],
       [4, 5, 6]])
&gt;&gt;&gt; <strong>a.fill(0)</strong>   <span class=comment># すべての要素を 0 にする。</span>
</pre></blockquote>

<p>
<code>ndarray</code>独自の (Python の多次元配列にはない) 機能として、
配列の部分列を <strong>2次元</strong>で切り出す機能がある。
これは通常の多次元配列の仕組みとは異なるため、添え字に
<code>,</code> を使った表記でないと実現できないことに注意。
この機能は、後に畳み込みニューラルネットワークを実装する際に活用する。
<blockquote>
<pre>
&gt;&gt;&gt; <strong>a = np.array([[1,2,3], [4,5,6], [7,8,9]])</strong>
&gt;&gt;&gt; <strong>a[0:2,1:3]</strong>   <span class=comment># (0〜1行×1〜2列)目を2次元で切り出す。</span>
array([[2, 3],
       [5, 6]])
&gt;&gt;&gt; <strong>a[0:2][1:3]</strong>  <span class=comment># これではうまくいかない (a[1:2]と同じ結果)。</span>
array([[4, 5, 6]])
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="140" height="130">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="black" stroke-width="1">
  <rect x="20" y="20" width="90" height="90" />
  <path d="M20,50 l90,0 M20,80 l90,0 M50,20 l0,90 M80,20 l0,90" />
  <path d="M50,115 l0,10 m0,-5 l15,0 m30,0 l15,0 m0,-5 l0,10" stroke-width="2" />
  <path d="M115,20 l10,0 m-5,0 l0,20 m0,20 l0,20 m-5,0 l10,0" stroke-width="2" />
  <rect x="50" y="20" width="60" height="60" stroke-width="3" />
</g>
<g style="font-family: courier; font-style: italic; font-size: 20px;" text-anchor="middle">
  <text x="35" y="35" dy="0.5em">1</text>
  <text x="65" y="35" dy="0.5em">2</text>
  <text x="95" y="35" dy="0.5em">3</text>
  <text x="35" y="65" dy="0.5em">4</text>
  <text x="65" y="65" dy="0.5em">5</text>
  <text x="95" y="65" dy="0.5em">6</text>
  <text x="35" y="95" dy="0.5em">7</text>
  <text x="65" y="95" dy="0.5em">8</text>
  <text x="95" y="95" dy="0.5em">9</text>
</g>
<g style="font-family: courier; font-size: 75%;" text-anchor="middle">
  <text x="80" y="115" dy="0.5em">1:3</text>
  <text x="125" y="50" dy="0.5em">0:2</text>
</g>
<g style="font-size: 75%;" text-anchor="middle">
  <text x="20" y="10" dy="0.5em">0</text>
  <text x="50" y="10" dy="0.5em">1</text>
  <text x="80" y="10" dy="0.5em">2</text>
  <text x="110" y="10" dy="0.5em">3</text>
  <text transform="translate(10,20) rotate(270)" dy="0.5em">0</text>
  <text transform="translate(10,50) rotate(270)" dy="0.5em">1</text>
  <text transform="translate(10,80) rotate(270)" dy="0.5em">2</text>
  <text transform="translate(10,110) rotate(270)" dy="0.5em">3</text>
</g>
</svg>
</div></blockquote>

</blockquote>

<p>
<code>ndarray</code>配列の大きさを知るためには複数の方法がある:
<blockquote><pre>
&gt;&gt;&gt; <strong>a = np.array([[1,2,3], [4,5,6]])</strong>
&gt;&gt;&gt; <strong>len(a)</strong>   <span class=comment># リストとして見たときの要素数 (行数)。</span>
2
&gt;&gt;&gt; <strong>a.size</strong>   <span class=comment># 全要素数。</span>
6
&gt;&gt;&gt; <strong>a.shape</strong>  <span class=comment># 配列の「形状」。</span>
(2, 3)
</pre></blockquote>

<p>
さて、<code>ndarray</code>配列には要素を追加・削除することはできないが、
<code>reshape()</code> メソッドを使って「形を変える」ことはできる。
これは後でデータを効率よく処理したいときに使える:
<blockquote><pre>
&gt;&gt;&gt; <strong>a = np.array([[1,2,3], [4,5,6]])</strong>  # もとは 2行×3列の配列。
&gt;&gt;&gt; <strong>a.reshape(3,2)</strong>  <span class=comment># 3行×2列の配列に変換。</span>
array([[1, 2],
       [3, 4],
       [5, 6]])
&gt;&gt;&gt; <strong>a.reshape(6)</strong>    <span class=comment># フラットな1次元配列に変換。</span>
array([1, 2, 3, 4, 5, 6])
</pre></blockquote>

<h3 id="numpy-utils">3.5. ndarray型を便利に使うための関数</h3>
<p>
他にも、NumPy には <code>ndarray</code>配列を便利に使うための
関数がいくつも用意されている。代表的なものを以下に示す:
<blockquote><pre>
&gt;&gt;&gt; <strong>np.sum(np.array([1, 2, 3]))</strong>                  <span class=comment># 1+2+3 を計算。</span>
6
&gt;&gt;&gt; <strong>np.sqrt(np.array([1, 2, 3]))</strong>                 <span class=comment># sqrt(1), sqrt(2), sqrt(3) を計算。</span>
array([1., 1.41421356, 1.73205081])
&gt;&gt;&gt; <strong>np.exp(np.array([1, 2, 3]))</strong>                  <span class=comment># exp(1), exp(2), exp(3) を計算。</span>
array([2.71828183, 7.3890561, 20.08553692])
&gt;&gt;&gt; <strong>np.dot(np.array([1,2,3]), np.array([4,5,6])</strong>  <span class=comment># 内積 (1*4 + 2*5 + 3*6) を計算。</span>
32
&gt;&gt;&gt; <strong>np.max(np.array([5, 9, 4, 0]))</strong>               <span class=comment># もっとも大きな要素。</span>
9
&gt;&gt;&gt; <strong>np.argmax(np.array([5, 9, 4, 0]))</strong>            <span class=comment># もっとも大きな要素の<strong>添字</strong>。</span>
1
</pre></blockquote>

<h2 id="nn-numpy">4. NumPyを使ったニューラルネットワークの実装</h2>
<p>
では前回の Layerクラスを <code>ndarray</code>型を用いて書き直してみよう。
<p>
インスタンスの初期化部分では、Python のリスト内包表記を
<code>ndarray</code>型の作成にすればよい。
ここで定義される <code>self.w</code>, <code>self.dw</code> は、
それぞれ <code>nout</code>行×<code>nin</code>列の2次元配列となり、
<code>self.b</code>, <code>self.db</code> はそれぞれ
<code>nout</code>要素の配列となる
(以前の部分は薄灰色で表示されている):
<blockquote><pre>
class Layer:
    def __init__(self, nin, nout):
        self.nin = nin
        self.nout = nout
        <span class=comment># 重み・バイアスを初期化する。</span>
        <span class=prev># self.w = [ [ random()-.5 for j in range(self.nin) ] for i in range(self.nout) ]</span>
        <span class=prev># self.b = [ random()-.5 for i in range(self.nout) ]</span>
        <mark>self.w = np.random.random((self.nout, self.nin)) - .5</mark>
        <mark>self.b = np.random.random(self.nout) - .5</mark>
        <span class=comment># 計算用の変数を初期化する。</span>
        self.x = self.y = None
        <span class=prev># self.dw = [ [ 0 for j in range(self.nin) ] for i in range(self.nout) ]</span>
        <span class=prev># self.db = [ 0 for i in range(self.nout) ]</span>
        <mark>self.dw = np.zeros((self.nout, self.nin))</mark>
        <mark>self.db = np.zeros(self.nout)</mark>
        self.loss = 0
        return
</pre></blockquote>

<p>
<code>forward()</code> メソッドは、以下のように変更する:
<blockquote><pre>
    def forward(self, x):
        <span class=comment># xは nin個の要素をもつ入力値のリスト。</span>
        <span class=comment># 与えられた入力に対する各ノードの出力を計算する。</span>
        self.x = x
        <span class=prev># self.y = [</span>
        <span class=prev>#     sigmoid(sum( w1*x1 for (w1,x1) in zip(w, x) ) + b)</span>
        <span class=prev>#     for (w,b) in zip(self.w, self.b)</span>
        <span class=prev># ]</span>
        <mark>self.y = sigmoid(np.dot(self.w, x) + self.b)</mark>
        <span class=comment># yは nout個の要素をもつ出力値のリスト</span>
        return self.y
</pre></blockquote>
<p>
ここでは出力の計算を <code>np.dot()</code> を使って 1行でおこなっている:
<blockquote>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="450" height="120">
<defs>
  <marker id="barrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="blue" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="blue" stroke-width="2">
  <rect x="35" y="35" width="175" height="20" />
  <rect x="35" y="65" width="175" height="20" />
  <rect x="35" y="95" width="175" height="20" />
  <rect x="280" y="35" width="155" height="20" />
  <path d="M280,40 c-20,-15,-40,-15,-65,-5" marker-end="url(#barrow)" />
  <path d="M280,40 c-20,-5,-40,-5,-65,25" marker-end="url(#barrow)" />
  <path d="M280,40 c-20,5,-40,10,-65,50" marker-end="url(#barrow)" />
</g>
<g fill="none" stroke="black" stroke-width="2">
  <path d="M32,30 l0,-5 l70,0 m40,0 l70,0 l0,5" />
  <path d="M275,30 l0,-5 l60,0 m40,0 l60,0 l0,5" />
  <path d="M22,35 l-5,0 l0,30 m0,20 l0,30 l5,0" />
</g>
<g style="font-size: 75%;">
  <text x="124" y="30" text-anchor="middle">nin</text>
  <text x="360" y="30" text-anchor="middle">nin</text>
  <text x="0" y="80">nout</text>
  <text fill="blue" x="230" y="20">np.dot</text>
</g>
<g style="font-family: courier; font-size: 20px;">
  <text x="120" y="15" text-anchor="middle">self.w</text>
  <text x="360" y="15" text-anchor="middle">x</text>
  <text x="20" y="50">[[w11, w12, ...],</text>
  <text x="30" y="80">[w21, w22, ...],</text>
  <text x="165" y="110">... ]</text>
  <text x="280" y="50">[x1, x2, ...]</text>
</g>
</svg><br>
<code>np.dot(self.w, x)</code><br>
右側の <code>x</code> が左側の <code>self.w</code> の各要素に分配され
(<span class=bc>broadcast</span>)、それぞれの np.dot が計算される。
</div></blockquote>

<p>
なお NumPy 用の <code>sigmoid()</code> 関数は、以下のようにする。
これは <code>ndarray</code>型の各要素に対して
シグモイド関数を計算するような関数となっている:
<blockquote><pre>
def sigmoid(x):
    <span class=prev># return 1 / (1 + exp(-x))</span>
    return 1 / (1 + np.exp(-x))
def d_sigmoid(y):
    return y * (1-y)
</pre></blockquote>

<p>
<code>mse_loss()</code>, <code>backward()</code>, <code>update()</code> の
各メソッドも同様に書き直す。<code>ndarray</code> を使うことにより
簡潔になっている部分に注目してほしい:
<blockquote><pre>
    def mse_loss(self, ya):
        <span class=comment># 与えられた正解に対する損失を求める。</span>
        <span class=prev># self.loss += sum( (y1-ya1)**2 for (y1,ya1) in zip(self.y, ya) )</span>
        <mark>self.loss += np.sum((self.y - ya)**2)</mark>
        <span class=comment># 損失関数の微分を計算する。</span>
        <span class=prev># delta = [ 2*(y1-ya1) for (y1,ya1) in zip(self.y, ya) ]</span>
        <mark>delta = 2*(self.y - ya)</mark>
        return delta
</pre><pre>
    def backward(self, delta):
        <span class=comment># self.y が計算されたときのシグモイド関数の微分を求める。</span>
        <span class=prev># ds = [ d_sigmoid(y1) for y1 in self.y ]</span>
        <mark>ds = d_sigmoid(self.y)</mark>
        <span class=comment># 各偏微分を計算する。</span>
        <span class=prev># for i in range(self.nout):</span>
        <span class=prev>#     for j in range(self.nin):</span>
        <span class=prev>#         self.dw[i][j] += delta[i] * ds[i] * self.x[j]</span>
        <span class=prev># for i in range(self.nout):</span>
        <span class=prev>#     self.db[i] += delta[i] * ds[i]</span>
        <mark>self.dw += (delta * ds).reshape(self.nout, 1) * self.x</mark>
        <mark>self.db += delta * ds</mark>
        <span class=comment># 各入力値の微分を求める。</span>
        <span class=prev># dx = [</span>
        <span class=prev>#     sum( delta[j]*ds[j]*self.w[j][i] for j in range(self.nout) )</span>
        <span class=prev>#     for i in range(self.nin)</span>
        <span class=prev># ]</span>
        <mark>dx = np.dot(delta * ds, self.w)</mark>
        return dx
</pre><pre>
    def update(self, alpha):
        <span class=comment># 現在の勾配をもとに、損失が減る方向へ重み・バイアスを変化させる。</span>
        <span class=prev># for i in range(self.nout):</span>
        <span class=prev>#     for j in range(self.nin):</span>
        <span class=prev>#         self.w[i][j] -= alpha * self.dw[i][j]</span>
        <span class=prev># for i in range(self.nout):</span>
        <span class=prev>#     self.b[i] -= alpha * self.db[i]</span>
        <mark>self.w -= alpha * self.dw</mark>
        <mark>self.b -= alpha * self.db</mark>
        <span class=comment># 計算用の変数をクリアしておく。</span>
        <span class=prev># for i in range(self.nout):</span>
        <span class=prev>#     for j in range(self.nin):</span>
        <span class=prev>#         self.dw[i][j] = 0</span>
        <span class=prev># for i in range(self.nout):</span>
        <span class=prev>#     self.db[i] = 0</span>
        <mark>self.dw.fill(0)</mark>
        <mark>self.db.fill(0)</mark>
        self.loss = 0
        return
</pre></blockquote>

<div class=exercise id="ex4-3">
<div class=header>演習4-3. NumPyを使ったニューラルネットワークを実行する</div>
<p>
上の NumPy を使ったニューラルネットワークの実装を使って、
前回のピタゴラスの定理を学習する例題を実行せよ。
<blockquote><pre>
<span class=comment># 100個分のランダムな訓練データを作成する。</span>
<span class=comment># つねに乱数値を一定にする。</span>
np.random.seed(0)
data = []
for i in range(100):
    x = np.random.random(3)   <span class=comment># 入力</span>
    ya = np.sqrt((x**2) / 3)  <span class=comment># 正解</span>
    data.append((x, ya))

layer1 = Layer(3, 3)
layer2 = Layer(3, 1)
<span class=comment># ... 以下は同じ ...</span>
</pre></blockquote>
</div>
<p>
実際に実行してみると NumPy を使ったバージョンのほうが
若干遅いことに気づくが、これは <code>ndarray</code>の処理にかかる
オーバーヘッドがあるためで、扱っている要素数が少ない場合はこのようになる。
配列の要素が増えると NumPy のほうが速くなる。


<h2 id="mnist-impl">5. MNIST を実装する</h2>
<p>
では NumPy で実装したニューラルネットワークを使って、
実際に MNIST の認識タスクをやってみよう。
訓練データおよびテストデータは
<a target="_blank" href="http://yann.lecun.com/exdb/mnist/">MNISTページ</a>からダウンロードする。
4つの .gz形式のファイルを保存すればよい。
(<strong>注意: </strong>
macOS などでは、.gz形式のファイルが勝手に展開されてしまうことがあるので、
リンクを <kbd>Control</kbd> + クリック して
<kbd>リンク先のファイルをダウンロード</kbd> を選ぶ。)
<ul>
<li> <code>train-images-idx3-ubyte.gz</code>
<li> <code>train-labels-idx1-ubyte.gz</code>
<li> <code>t10k-images-idx3-ubyte.gz</code>
<li> <code>t10k-labels-idx1-ubyte.gz</code>
</ul>

<p>
今回作成するのは次のような 3層の (<code>Layer</code> を 2つ使った)
ニューラルネットワークである:
<blockquote><pre>
layer1 = Layer(784, 100)  <span class=comment># 784個の入力、100個の出力。</span>
layer2 = Layer(100, 10)   <span class=comment># 100個の入力、10個の出力。</span>
</pre>
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="480" height="200">
<defs>
  <marker id="arrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="black" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="black" stroke-width="2">
  <circle cx="80" cy="20" r="15" />
  <circle cx="80" cy="60" r="15" />
  <circle cx="80" cy="140" r="15" />
  <circle cx="80" cy="180" r="15" />
  <circle cx="140" cy="40" r="15" />
  <circle cx="140" cy="80" r="15" />
  <circle cx="140" cy="120" r="15" />
  <circle cx="140" cy="160" r="15" />
  <circle cx="200" cy="60" r="15" />
  <circle cx="200" cy="140" r="15" />
  <g marker-end="url(#arrow)">
    <line x1="95" x2="125" y1="20" y2="30" />
    <line x1="95" x2="125" y1="20" y2="65" />
    <line x1="95" x2="125" y1="20" y2="105" />
    <line x1="95" x2="125" y1="60" y2="40" />
    <line x1="95" x2="125" y1="60" y2="75" />
    <line x1="95" x2="125" y1="60" y2="110" />
    <line x1="95" x2="125" y1="140" y2="85" />
    <line x1="95" x2="125" y1="140" y2="120" />
    <line x1="95" x2="125" y1="140" y2="155" />
    <line x1="95" x2="125" y1="180" y2="95" />
    <line x1="95" x2="125" y1="180" y2="125" />
    <line x1="95" x2="125" y1="180" y2="160" />
    <line x1="155" x2="185" y1="40" y2="55" />
    <line x1="155" x2="185" y1="40" y2="90" />
    <line x1="155" x2="185" y1="80" y2="60" />
    <line x1="155" x2="185" y1="80" y2="95" />
    <line x1="155" x2="185" y1="120" y2="100" />
    <line x1="155" x2="185" y1="120" y2="135" />
    <line x1="155" x2="185" y1="160" y2="105" />
    <line x1="155" x2="185" y1="160" y2="140" />
  </g>
  <path d="M60,5 l-10,0 l0,190 l10,0" />
  <path d="M125,25 l-10,0 l0,150 l10,0" />
  <path d="M220,45 l10,0 l0,110 l-10,0" />
</g>
<rect x="40" y="85" width="200" height="30" fill="white" />
<g text-anchor="middle" style="font-size:150%;">
  <text transform="translate(80,100) rotate(90)" x="0" y="0">...</text>
  <text transform="translate(140,100) rotate(90)" x="0" y="0">...</text>
  <text transform="translate(200,100) rotate(90)" x="0" y="0">...</text>
</g>
<g style="font-size:75%;">
  <text x="35" y="105">784</text>
  <text x="100" y="105">100</text>
  <text x="220" y="105">10</text>
</g>
<image x="0" y="50" href="digit3.png" />
<g fill="none" stroke="black" stroke-width="4" marker-end="url(#arrow)">
  <path d="M25,70 l15,0" />
  <path d="M240,120 l15,0" />
</g>
<g style="font-family: courier;">
  <text x="265" y="125">[0 0 0 1 0 0 0 0 0 0]</text>
</g>
</svg><br>
MNIST 用ニューラルネットワークの構造
</div></blockquote>

<p>
ダウンロードした MNISTのデータを読み込むため、
<a href="mnist.py">mnist.py</a> ファイルで <code>load_mnist()</code> 関数が
定義されている。
これは、ファイルの内容をひとつの巨大な <code>ndarray</code>型に変換する。
以下のようにして訓練データ用の画像とラベルを取得する。
変数 <code>train_images</code> と <code>train_labels</code> の内容は、
それぞれ次のようになっている:
<blockquote><pre>
train_images = load_mnist('train-images-idx3-ubyte.gz')  <span class=comment># [<img src="digit3.png">, <img src="digit9.png">, ...]</span>
train_labels = load_mnist('train-labels-idx1-ubyte.gz')  <span class=comment># [3, 9, ...]</span>
</pre></blockquote>
<p>
どちらの変数にも、60,000個ずつの訓練データが含まれている。
<code>train_images</code> 中の各画像はさらに 28×28 の2次元配列であるので、
全体として <code>train_images</code> は 60,000×28×28要素 の <code>ndarray</code> となり、
<code>train_labels</code> は 60,000要素 の <code>ndarray</code> となっている。
これら2つの要素を <code>zip()</code> 関数で対にして、
画像とラベルをひとつずつニューラルネットワークに学習させていく:
<blockquote><pre>
    for (image,label) in zip(train_images, train_labels):
        <span class=comment># 28×28の画像を784要素のフラットな配列に変換。</span>
        x = (image/255).reshape(784)
        <span class=comment># 正解部分だけが 1 になっている 10要素の配列を作成。</span>
        ya = np.zeros(10)
        ya[label] = 1
        ...
</pre></blockquote>
<p>
ここでは各画像 <code>image</code> を255で割り、
<code>reshape()</code> でフラットな1次元配列に変換している。
これは元画像がグレースケールのため各ピクセルの範囲が 0〜255 の
整数となっているのを、 0〜1 の小数に変換するためである。
正解データ <code>ya</code> はラベルの値を元に 10要素の配列を作成している。
<p>
まとめると、次のようなプログラムになる:
<div class=file>
mnist_slow.py (遅いバージョン)
<pre>
<span class=comment># 訓練データの画像・ラベルを読み込む (パス名は適宜変更)。</span>
train_images = load_mnist('train-images-idx3-ubyte.gz')
train_labels = load_mnist('train-labels-idx1-ubyte.gz')
<span class=comment># レイヤーを 2つ作成。</span>
layer1 = Layer(784, 100)
layer2 = Layer(100, 10)
<span class=comment># 100回繰り返す。</span>
for i in range(100):
    for (image,label) in zip(train_images, train_labels):
        <span class=comment># 28×28の画像を784要素のフラットな配列に変換。</span>
        x = (image/255).reshape(784)
        <span class=comment># 正解部分だけが 1 になっている 10要素の配列を作成。</span>
        ya = np.zeros(10)
        ya[label] = 1
        <span class=comment># 損失・勾配を計算。</span>
        y = layer1.forward(x)
        y = layer2.forward(y)
        delta = layer2.mse_loss(ya)
        delta = layer2.backward(delta)
        delta = layer1.backward(delta)
   print(layer2.loss)
   layer1.update(0.01)
   layer2.update(0.01)
</pre>
</div>
<p>
実は上のプログラムをそのまま実行しても一応動くのだが、
非常に時間がかかってしまう。
そこで、以下にもうすこし実用的な方法を説明しよう。

<h3 id="mnist-minibatch">5.1. ミニバッチと SGD法</h3>
<p>
本来、勾配降下法は訓練データ全部に対する勾配の平均を使って
重み・バイアスを調整していく方法であった。
しかし MNIST の訓練データは非常に大きく、
これを一度処理するだけでも時間がかかる。
もしこれを数千回も反復させると、学習に非常に時間がかかることになる。
<p>
そこで、訓練データを全部見ずに、
一部を見た時点でパラメータを漸進的に更新していく、という方法が考えられる。
訓練データをいくつかの <u>ミニバッチ</u> (minibatch) に区切り、
ミニバッチごとに重み・バイアスを更新していくのである:
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="330" height="65">
<g fill="none" stroke="black" stroke-width="2">
  <rect x="5" y="20" width="320" height="40" />
  <rect x="10" y="25" width="70" height="30" />
  <rect x="85" y="25" width="70" height="30" />
  <rect x="160" y="25" width="70" height="30" />
  <path d="M5,15 l0,-5 l90,0 m140,0 l90,0 l0,5" />
</g>
<g style="font-size:75%;" text-anchor="middle">
  <text x="165" y="7" dy="0.5em">訓練データ全部 (バッチ)</text>
  <text x="45" y="40" dy="0.5em">ミニバッチ</text>
  <text x="120" y="40" dy="0.5em">ミニバッチ</text>
  <text x="195" y="40" dy="0.5em">ミニバッチ</text>
  <text x="240" y="40" dy="0.5em">...</text>
</g>
</svg>
</div>
<p>
このためには、たとえば上のプログラムを次のように変更すればよい:
<div class=file>
mnist_minibatch.py (ミニバッチバージョン)
<pre>
<span class=comment># カウンタを初期化する。</span>
<mark>n = 0</mark>
for (image,label) in zip(train_images, train_labels):
    <span class=comment># 1個のデータに対して損失・勾配を計算。</span>
    ...
    <mark>n += 1</mark>
    <span class=comment># 50個ごとに重み・バイアスを更新する。</span>
    <mark>if (n % 50) == 0:</mark>
    <mark>    print(layer2.loss)</mark>
    <mark>    layer1.update(0.01)</mark>
    <mark>    layer2.update(0.01)</mark>
</pre>
</div>
<p>
この例では、入力50個ごとに重み・バイアスを更新している。
MNIST の訓練データは全部で 60,000個あるので、
60000÷50 = 300回更新されることになる (300ミニバッチ)。
ミニバッチを使って重みを更新していくと、
訓練データを全部見ているわけではないので、
学習の結果は訓練データが現れる順序、
つまり<strong>確率</strong>にある程度左右されてしまう。
そのため、この方法は<u>確率的 勾配降下法</u>
(Stochastic Gradient Decent)、通称 <u>SGD</u> と呼ばれている。
実際には、全訓練データを1回見るだけでは学習が足りないので、
さらにこのプロセス全体を何回か繰り返す。
この「全訓練データに対する繰り返し回数」を<u>エポック</u> (epoch) という。
SGD は、今日のニューラルネットワークでは標準的な学習方法である。
<p>
たとえばエポックを 5 とすると:
<blockquote><pre>
<span class=comment># カウンタを初期化する。</span>
n = 0
<span class=comment># 全訓練データに対して5回繰り返す (5エポック) 。</span>
<mark>for epoch in range(5):</mark>
    for (image,label) in zip(train_images, train_labels):
        <span class=comment># 1個のデータに対して損失・勾配を計算。</span>
        ...
</pre></blockquote>
<p>
<strong>注意:</strong>
本来は 1エポックごとに訓練データの順序をシャッフルして、
なるべく訓練データの順序によって学習に偏りが出ないようにするべきである。
今回はその処理は省略している。

<div class=exercise id="ex4-4">
<div class=header>演習4-4. MNISTを学習する</div>
<ol type=a>
<li> 上のプログラム <code>mnist_minibatch.py</code> を完成させ、実際に実行せよ。
ただし、エポック 5回は実行時間が長すぎるので、最初は 1回でよい。
<li> エポックを 2回に増やすと、表示される損失はどう変化するか?
</ol>
</div>

<h3 id="mnist-argmax">5.2. 学習結果を使う</h3>
<p>
学習が完了したら、実際にそのニューラルネットワークを使って認識をさせてみよう。
テストデータを読み込み、これをいま学習したネットワークに通す。
本来、出力である 10要素のベクトルには、該当する数字の要素が
「<code>1</code>」になっているはずだが、
計算結果が正確に 1 になることはまずありえないので、
ここでは「もっとも大きな値」の要素を正解の数字とする。
このためには、<code>np.argmax()</code> 関数を使う:
<blockquote><pre>
&gt;&gt;&gt; <strong>y = np.array([0.1, 0.0, 0.2, 0.9, 0.1, 0.1, 0.5, 0.2, 0.6, 0.4])</strong>
&gt;&gt;&gt; <strong>np.argmax(y)</strong>
3
</pre></blockquote>
<p>
実際に認識精度を測定する部分は、次のようになる:
<blockquote><pre>
<span class=comment># テストデータを使って認識精度を測定する。</span>
test_images = load_mnist('t10k-images-idx3-ubyte.gz')
test_labels = load_mnist('t10k-labels-idx1-ubyte.gz')
correct = 0  <span class=comment># 正解した数。</span>
for (image,label) in zip(test_images, test_labels):
    x = (image/255).reshape(784)
    <span class=comment># ニューラルネットワークで推論をおこなう。</span>
    y = layer1.forward(x)
    y = layer2.forward(y)
    <span class=comment># 10要素の出力ベクトルのうち、値がもっとも大きな要素を選ぶ。</span>
    i = np.argmax(y)
    if i == label:
        correct += 1  <span class=comment># label と等しければ正解。</span>
<span class=comment># テストデータの数と、正解した数とを表示する。</span>
print(len(test_labels), correct)
</pre></blockquote>

<div class=exercise id="ex4-5">
<div class=header>演習4-5. MNISTの正解数を測定する</div>
<ol type=a>
<li> <a href="#ex4-4">演習4-4.</a> で完成させた <code>mnist_minibatch.py</code> の
  末尾に上のコードを追加し、認識精度を測定せよ。
<li> エポックが 1回のときと 2回のときで認識精度の違いを観察せよ。
<li> レイヤー1 と レイヤー2 の間に、さらに 100ノードの中間レイヤーを
  挿入すると、認識精度はどう変化するか?
<pre>
layer1 = Layer(784, 100)
<mark>layerx = Layer(100, 100)</mark>
layer2 = Layer(100, 10)
</pre>
</ol>
</div>

<li>
ここで注目すべきことは、レイヤーをいくら増やしても
ネットワーク全体は依然として<strong>微分可能</strong>になっているということである。
一般に、レイヤーを増やせば増やすほどニューラルネットワークの
学習能力は向上すると言われている。
しかし同時に重み・バイアスが変化する速度も遅くなるため
(<u>勾配消失問題</u>)、より学習に時間がかかるようになる。


<h3 id="mnist-validation">5.3. エポックは何回やれば充分か?</h3>
<p>
これまでエポックの回数を多くすればニューラルネットワークが収束し、
損失が減少すると説明してきた。
ではエポックは多くすればするほどよいのだろうか? 実はそうではない。
<strong>損失が少ないからといって、誤りが少ないとは限らない</strong>のである。
ニューラルネットワークの損失が少なすぎると、
これは訓練データの<strong>正解をただ記憶しているだけ</strong>になってしまう。
このような現象を<u>過学習</u> (overfitting) という。
過学習は機械学習システムが「過去問 (訓練データ)」に適合することにばかり
注力してしまい、「本番の試験 (テストデータ)」に対応できなくなって
しまっている状態である。過学習はどのような機械学習システムでも存在しうるが、
ニューラルネットワークの場合、これはエポックを多くしすぎると発生する。
<div class=figure>
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.1" width="200" height="150">
<defs>
  <marker id="arrow" viewBox="-5 -5 10 10" orient="auto">
    <polygon points="-5,-5 5,0 -5,5" fill="black" stroke="none" />
  </marker>
</defs>
<g fill="none" stroke="black" stroke-width="2">
  <line x1="10" y1="140" x2="10" y2="10" marker-end="url(#arrow)" />
  <line x1="10" y1="140" x2="140" y2="140" marker-end="url(#arrow)" />
  <path d="M20,20 c10,20,30,90,80,90 c20,0,40,-10,80,-20" stroke-width="1" stroke-dasharray="2,2" />
  <path d="M20,30 c30,90,50,100,150,100" stroke-width="1" />
</g>
<g fill="none" stroke="red" stroke-width="1">
  <ellipse cx="150" cy="100" rx="25" ry="10" />
</g>
<g style="font-size:75%;">
<text x="30" y="25" dy="0.5em">誤り</text>
<text x="15" y="85" dy="0.5em">損失</text>
<text x="130" y="80" dy="0.5em" fill="red">過学習</text>
<text x="195" y="140" dy="0.5em" text-anchor="end">エポック</text>
</g>
</svg><br>
損失が減少しても、誤りは増加する (過学習)
</div>
<p>
このような事態を防ぐために使われているのが <u>検証データ</u> (validation data)
である。これは訓練データの一部をあらかじめ小規模なテストデータとして
分けておき、エポックごとに毎回小規模な精度測定をおこなって
精度が低下していないかチェックする、というアイデアである。
検証データを使った訓練は、以下のようなステップでおこなう:
<ol>
<li> 訓練データを「訓練データ」と「検証データ」に分ける。
<li> 毎エポックごとに、訓練データでニューラルネットワークを訓練したあと、
検証データを使って簡単に精度測定をおこなう。
<li> 精度が改善している場合は、2. を繰り返す。
</ol>
<p>
以上を疑似コードで表すと以下のようになる:
<blockquote><pre>
prev_accuracy = 0
for epoch in range(100):
    <span class=comment># 訓練データで訓練する。</span>
    train(train_data)
    <span class=comment># 検証データで精度を測定する。</span>
    accuracy = test(val_data)
    <span class=comment># 前よりも悪化していたら、そこで訓練をやめる。</span>
    if accuracy &lt;= prev_accuracy:
        break
    prev_accuracy = accuracy
</pre></blockquote>
<p>
本講座では検証データを使った実装は省略するが、
精度の高いニューラルネットワークを実際に設計・訓練しようとする際には
検証データは必要不可欠である。


<h3 id="mnist-softmax">5.4. Softmax 活性化関数と交差エントロピー損失</h3>
<p>
以上で MNIST をニューラルネットワークで実装することができた。
<a href="#ex4-5">演習4-5.</a> をやってみると、おそらく 92% 程度の精度が出るはずである。
これは単純なニューラルネットワークにしては、そこそこの性能といえるかもしれないが、
まだそれほど高いとはいえない。
<p>
認識精度をさらに上げるには、どうすればよいだろうか?
これまで、精度を上げる方法として以下のようなものを見てきた:
<ol type=a>
<li> 訓練データの数を増やす。
<li> 各レイヤーごとのノードの数を増やす。
<li> レイヤーの数 (深さ) を増やす。
<li> 反復 (エポック) の数を増やす。
</ol>
<p>
実はニューラルネットワークの世界では、これ以外にも精度を上げるための
数多くのテクニックがある。ここではそのひとつとして
「損失関数を改良する」方法を説明する。
<p>
これまでのニューラルネットワークでは、
損失関数として平均二乗誤差 (MSE loss) を使ってきた。
平均二乗誤差を下げるということは、ニューラルネットワークの出力が
訓練データの正解に近づくということを意味する。
しかし、MNIST のようなタスクの損失として平均二乗誤差を使うことは
必ずしも理想的ではない。たとえば、以下のようなケースがあったとする:
<pre>
<strong>出力:</strong> [ 0.1, 0.0, 0.2, 0.9, 0.1, 0.1, 0.5, 0.2, 0.6, 0.4]
<strong>正解:</strong> [   0,   0,   0,   1,   0,   0,   0,   0,   0,   0]
</pre>
<p>
まず、このニューラルネットワークの出力ベクトルのうち、
重要なのは「何番目の要素が最大か」という情報だけである。
「<code>0.9</code>」などの具体的な値は重要ではない。
また、正解ベクトルでも重要なのは「何番目の要素が最大か」
という情報だけであって、<code>1</code> という具体的な値が重要なわけではない。
しかし平均二乗誤差を使ったニューラルネットワークではとにかく
出力ベクトルの個々の値を正解ベクトルに近づけようとするため、
これらの余計な情報まで学習しようとする。
<p>
さらに、MNISTのようなタスクでは、ひとつの入力画像に対する正解は 1つだけである。
つまり、正解ベクトルの 2箇所以上が <code>1</code> になることはない。
しかしニューラルネットワークはこの特性を利用できておらず、
出力のすべての数値になんらかの意味があるものとして学習している。
<p>
以上の 2つの問題を一気に解決するのが
「Softmax 活性化関数」と「交差エントロピー損失」である。
これは MNIST のように「複数のカテゴリから 1つのものを選ぶ」ような
判定タスクに使われる。その具体的な方法は以下のとおりである:
<ol>
<li> ニューラルネットワークの最後のレイヤーだけ、
  シグモイド関数のかわりに <u>Softmax関数</u> を活性化関数として使う。
<li> 損失関数として、平均二乗誤差のかわりに <u>交差エントロピー誤差</u>
  (cross entropy error) というものを使う。
</ol>
<p>
Softmax関数とは、文字どおり最大値を返す max関数を
「ソフトに」したものである。微分可能な max関数といってもよい。
これは与えられたベクトルの各要素 a<sub>i</sub> を
以下のような値で置き換える:
<div class=formula>
a<sub>i</sub> → exp(a<sub>i</sub>) / (exp(a<sub>1</sub>) + exp(a<sub>2</sub>) + ... + exp(a<sub>n</sub>))
</div>
<p>
通常の Python では、これは以下のように書ける:
<blockquote><pre>
def softmax(x):
    x = [ exp(a) for a in x ]
    z = sum(x)
    return [ a/z for a in x ]
</pre></blockquote>
NumPy版では:
<blockquote><pre>
def softmax(x):
    x = np.exp(x)
    return x / np.sum(x)
</pre></blockquote>
<p>
いっぽう交差エントロピーとは、
本来は 2つの確率分布の差を求めるものである。
じつは Softmax関数の各要素を合計すると 1 になるので、
Softmax関数の出力は確率分布として解釈できるといってもよい。
正解データも同様にひとつの要素だけが <code>1</code> で、
あとはすべて <code>0</code> なので、
確率分布としての解釈が可能である。
<p>
n種類の可能性の確率をあらわす、ふたつの確率分布
[p<sub>1</sub>, p<sub>2</sub>, ..., p<sub>n</sub>] および
[q<sub>1</sub>, q<sub>2</sub>, ..., q<sub>n</sub>] があるとき、
交差エントロピー誤差 H(p, q) は、次のように表される:
<div class=formula>
H(p, q) = -(p<sub>1</sub> &middot; log(q<sub>1</sub>) +
p<sub>2</sub> &middot; log(q<sub>2</sub>) + ... +
p<sub>n</sub> &middot; log(q<sub>n</sub>))
</div>

<div class=exercise id="ex4-6">
<div class=header>演習4-6. Softmax関数と交差エントロピー誤差を求める</div>
<ol type=a>
<li> あるニューラルネットワークの重み合計が <code>[3, -2, 0, 9, 4]</code>
  だったとする。これらの値に対して Softmax関数を適用したときの値を求めよ。
<li> さらに、この値と <code>[0.1, 0.1. 0.5, 0.2, 0.1]</code> という
  確率分布との交差エントロピー誤差を求めよ。
</ol>
</div>

<p>
MNISTの判定においては、正解データのベクトルは
ひとつの要素だけが <code>1</code> である。
したがって、これを上の式に適用すると、ひとつの項だけが残り、
交差エントロピー誤差の計算は Python で以下のように簡単化できる:
<blockquote><pre>
def nll_loss(y, i):
    return -np.log(y[i])
</pre></blockquote>
<p>
ここで <code>y</code> は Softmaxレイヤーの出力、<code>i</code> は正解の要素である。
この関数は一般に <u>Negative Log Likelihood</u> (NLL) とも呼ばれる。
<p>
さて、Softmax関数と交差エントロピー誤差を同時に紹介したのには理由がある。
「Softmax関数 + 交差エントロピー誤差」の組み合わせを使うと、
<strong>勾配を計算するのが非常に簡単</strong>になるためである。
<p>
計算の詳細は省略するが、
交差エントロピー誤差の損失を L<sub>CEE</sub> とすると、
以下のようになる:
<div class=formula>
<span class=sym>&part;</span>L<sub>CEE</sub>/<span class=sym>&part;</span>w<sub>i</sub> =
<span class=sym>&part;</span>L<sub>CEE</sub>/<span class=sym>&part;</span>y &middot; <span class=sym>&part;</span>y/<span class=sym>&part;</span>w<sub>ij</sub> =
(y - y0) &middot; x<sub>i</sub>
<br>
<span class=sym>&part;</span>L<sub>CEE</sub>/<span class=sym>&part;</span>b =
<span class=sym>&part;</span>L<sub>CEE</sub>/<span class=sym>&part;</span>y &middot; <span class=sym>&part;</span>y/<span class=sym>&part;</span>b<sub>i</sub> =
(y - y0)
</div>

<p>
では実際に Softmax関数と交差エントロピー誤差を使った
新しいレイヤークラス <code>SoftmaxLayer</code> を定義しよう。
<code>__init__()</code> メソッドと <code>update()</code> メソッドは
以前と同じである:
<blockquote><pre>
<span class=comment># 入力 nin個、出力 nout個のSoftmaxレイヤーを定義する。</span>
class SoftmaxLayer:

    def __init__(self, nin, nout):
        ...

    def update(self, alpha):
        ...
</pre></blockquote>
<p>
<code>forward()</code> メソッドでは、
シグモイド関数のかわりに Softmax関数を使うよう変更する:
<blockquote><pre>
    def forward(self, x):
        <span class=comment># xは nin個の要素をもつ入力値のリスト。</span>
        <span class=comment># 与えられた入力に対する各ノードの出力を計算する。</span>
        self.x = x
        self.y = <mark>softmax</mark>(np.dot(self.w, x) + self.b)
        <span class=comment># yは nout個の要素をもつ出力値のリスト</span>
        return self.y
</pre></blockquote>
<p>
最後に、損失関数の計算および勾配降下法の部分を実装する。
先に説明した「Softmax関数 + 交差エントロピー誤差」で勾配計算が
簡単化されたため、これはひとつのメソッドでやってしまうことにする:
<blockquote><pre>
    def cross_entropy_loss_backward(self, ya):
        <span class=comment># 与えられた正解に対する損失を求める。</span>
        <mark>i = np.argmax(ya)</mark>
        self.loss += <mark>nll_loss(self.y, i)</mark>
        <span class=comment># 損失関数の微分を計算する。</span>
        delta = (self.y - ya)
        <span class=comment># 各偏微分を計算する。</span>
        self.dw += delta.reshape(self.nout, 1) * self.x
        self.db += delta
        <span class=comment># 各入力値の微分を求める。</span>
        dx = np.dot(delta, self.w)
        return dx
</pre></blockquote>

<div class=exercise id="ex4-7">
<div class=header>演習4-7. Softmaxレイヤーを使った MNIST</div>
<p>
<a href="#ex4-5">演習4-5.</a> で作成したコードの最後のレイヤーを
<code>SoftmaxLayer</code> に変更し、認識精度の違いを確認せよ:
<pre>
layer1 = Layer(784, 100)        <span class=comment># 784個の入力、100個の出力。</span>
layer2 = SoftmaxLayer(100, 10)  <span class=comment># 100個の入力、10個の出力。(Softmax + 交差エントロピー誤差)</span>
</pre>
</div>

<h3 id="mnist-logsoftmax">5.5. LogSoftmax関数</h3>
<p>
ちなみに第6章で説明する PyTorch では効率のため、
普通の Softmax関数ではなく、そのlogをとったものを使っている。
<div class=formula>
<table><tr>
<td>LogSoftmax(a<sub>i</sub>)</td>
<td>=</td>
<td>log( exp(a<sub>i</sub>) / (<span class=sym>&Sigma;</span> exp(a<sub>i</sub>) )</td>
</tr><tr>
<td></td>
<td>=</td>
<td>a<sub>i</sub> - log(<span class=sym>&Sigma;</span> exp(a<sub>i</sub>))</td>
</tr></table>
</div>
<p>
NumPy ではこのようなコードになる:
<blockquote><pre>
def log_softmax(x):
    return x - np.log(np.sum(np.exp(x)))
</pre></blockquote>
<p>
LogSoftmax を使うと、交差エントロピー誤差の計算
(Negative Log Likelihood) が簡単になる:
<blockquote><pre>
def nll_loss(y, i):
    <span class=prev># return -np.log(y[i])</span>
    return -y[i]
</pre></blockquote>


<h2 id="summary">6. まとめ</h2>
<ul>
<li> <u>ディープ ニューラル ネットワーク</u> あるいは <u>ディープ ラーニング</u> は、
多数の<nobr><span class=bl>レイヤー</span></nobr>を重ねたニューラルネットワークである。
<li> ディープニューラルネットワークも依然として<nobr><span class=bl>微分</span></nobr>可能である。
<li> 一般にレイヤーを増やせば増やすほど学習能力は向上するが、
  同時に訓練時間も<nobr><span class=bl>増加</span></nobr>する。
<li> ディープニューラルネットワークにおける学習は、<nobr><span class=bl>確率的</span></nobr>勾配降下法 (SGD) を
使っておこなう。
<li> SGD では、訓練データを<nobr><span class=bl>ミニバッチ</span></nobr> と
呼ばれるかたまりに分けて、重み・バイアスを更新する。
<li> MNIST のような画像判定タスクでは、入力は2次元画像のピクセル値をそのまま使用し、
出力形式には <nobr><span class=bl>one-hot</span></nobr> ベクトルを使用する。
<li> 判定タスクの場合、最後のレイヤーの活性化関数として <u>Softmax</u> を使い、
損失関数として<nobr><span class=bl>交差エントロピー</span></nobr>誤差を使う。
<li> NumPy の<nobr><span class=bl><code>ndarray</code></span></nobr>型の特徴は、
<strong>分配</strong> (<span class=bc>broadcast</span>) と
<strong>要素ごとの計算</strong> (<span class=ew>element-wise</span>) である。
</ul>


<hr>
<div class=license>
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="クリエイティブ・コモンズ・ライセンス" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />この作品は、<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">クリエイティブ・コモンズ 表示 - 継承 4.0 国際 ライセンス</a>の下に提供されています。
</div>
<address>Yusuke Shinyama</address>
